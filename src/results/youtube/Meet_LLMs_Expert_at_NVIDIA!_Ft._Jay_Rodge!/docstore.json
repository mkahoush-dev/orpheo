{"docstore/data": {"8be2311a-a564-44b3-a7cd-298ba039a20f": {"__data__": {"id_": "8be2311a-a564-44b3-a7cd-298ba039a20f", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05704e0b-aa2f-4a43-97fc-b4e42a5c4977", "node_type": "1", "metadata": {}, "hash": "eb8fff203984e093149c3830cc175272c65f01beb0775e6522b15485a58ce55b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "not just happen faster but in the cloud with everything set up thanks to even Nims right Nvidia inference microservices every product that you use today has some form of generative AI so do you want to stick to just one AI app or just do you want to use generator a within all the existing apps so so there are two uh stages in any llm development pipeline one is in uh training and the second is inference there's a multiple step in order to like optimize a large language model to be production ready so hello everyone today we are in Nvidia office again we have Jay here who is expert in llms at Nvidia and we will be talking about what he thinks about the switch in the workforce especially For Engineers who are you know trained for years and years in data structures algorithms to prepare for big Tech there's an industry how we can evolve and prepare the engineers who are spending the full 100% from the age of 18 to 22 to give their best in their college and to learn the best Technologies and especially lot of coming in AI at Nvidia Nims lot to break down so hi Jay can you please introduce yourself sure hi herur hi everyone uh my name is Jay I am a developer Advocate uh at Envia focusing on large language model so my job is to educate developers on how they can accelerate uh llm workflows on Nvidia gpus so uh my background is in computer science so I did my undergrad uh from uh University of Pune in computer science and then came to the states uh for my masters in computer science with a focus in machine learning so I did couple of uh internships at startup and uh before uh coming to Nvidia I was doing a research internship at BMW uh doing computer vision and deep learning uh so where I got to use Nvidia software specifically it was called Deep Stream So basically accelerating video uh based uh version based computer version U on gpus and I also got to try out their Nvidia djx station so it's a uh workstation that includes four workstation level uh gpus uh So based on that I got to interview at Nvidia it went well and I started as a product marketing manager uh for deep learning software so I did that almost for three and a half years uh working uh and launching deep learning software uh so that was fun so I got to see the marketing side of software and I also got to work on Keynotes uh where our uh Jenson or CEO of Nvidia Jensen uh tries to showcase the latest and greatest at the Nvidia Flagship uh conference called GPU technology conference so uh but I wanted to be more Hands-On and more uh teaching oriented so that's why I took this role uh where I get to do some fun stuff and I really want to get to know you what what what are your goals here at Nvidia career-wise what do you think about what's happening what are you catching up to in llms gen correct correct yeah talking about uh goals I so I am a a passionate Tech Enthusiast I like Tech so I grew up watching all the Keynotes so uh working at Nvidia that was one of my goals about how how the keynote Cycle Works uh so I got to be part of it so that was fun but throughout the pro proc I was uh work working as a product marketing manager I was trying to see how can I communicate the new features uh to developers and part of which I was trying to come up with various strategies like will a blog or a video help with understanding this feature uh well to a developer I felt like I could be a educator as well and that's how I switched uh from product marketing manager to developer Advocate where I get to do all those stuff which is teaching uh through creating blogs uh creating technical videos and then going into conferences uh giving technical talks and teaching how a technology works and how I can help them uh and your blogs have reached like you know and have been cited in top research papers now yes yes yeah so that that has been my main motivation about educating uh developers and researchers on how they can um make their pipeline or research better by using Nvidia GP how did you figure out your passion for llms and how you got good at them correct so I think the passion for a deep learning started in my undergrad so back then there was a course called uh uh celf driving car Nano degree from Udacity and I was super passionate about deep learning because it was magic back then like understanding what's in the image that was great and taking it to a step further understanding how cell driving cars work and what are the algorithms behind it and it was mostly computer vision core computer vision and deep learning so that was my motivation and I enjoyed that uh aspect because it is also some form of art as well like figuring out like what should be the learning rate of the deep noodle Network and things like that so that was fun for me and that's how I got into deep learning uh and then llm as llm was evolving uh our team was also looking for someone who could educate developers about uh all the great stuff happening in the ecosystem", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05704e0b-aa2f-4a43-97fc-b4e42a5c4977": {"__data__": {"id_": "05704e0b-aa2f-4a43-97fc-b4e42a5c4977", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8be2311a-a564-44b3-a7cd-298ba039a20f", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "8db88086e4347b83baf80d8323cd40942c70dc66345b62d0503e7d9fac98f18e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3da9e42-42d1-4892-a064-20777ef24638", "node_type": "1", "metadata": {}, "hash": "25ea6d81a9f0eb4696856ecfc76684c5a187f2df688834e11dcbec781412c696", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "research better by using Nvidia GP how did you figure out your passion for llms and how you got good at them correct so I think the passion for a deep learning started in my undergrad so back then there was a course called uh uh celf driving car Nano degree from Udacity and I was super passionate about deep learning because it was magic back then like understanding what's in the image that was great and taking it to a step further understanding how cell driving cars work and what are the algorithms behind it and it was mostly computer vision core computer vision and deep learning so that was my motivation and I enjoyed that uh aspect because it is also some form of art as well like figuring out like what should be the learning rate of the deep noodle Network and things like that so that was fun for me and that's how I got into deep learning uh and then llm as llm was evolving uh our team was also looking for someone who could educate developers about uh all the great stuff happening in the ecosystem and how how can they do it on gpus so I was like I'm up for it I I want to do it yeah how it has evolved what has fascinated you the most what are you catching up to now I think the earlier uh the notion was in the market that it it is based on my understanding and my opinions that U model were getting bigger and better better and that was the notion that bigger is always better but as the time has evolved I think people are also trying to go towards a smaller model because running big models like trillion parameter or like billion uh upwards of 500 billion parameter is not sustainable uh uh on a production scale right so you need lots of compute lots of electricity uh yeah a lot more money what are the three biggest things you have learned by being in Nvidia and in AI space first of all uh everyone here is aware about what uh what is going in the ecosystem like everyone is up to date and everyone you talk to knows about what are the latest model in the ecosystem how are they different from each other so everyone has a technical understanding about how these thing works uh the second learning I think everyone is humble here uh uh talking specifically about Nvidia everyone is super smart but they are also vulnerable or just try to give their best so it inspires you in a way that uh everyone is giving their uh like more than 100 person so it inspires and people are crazy smart here so it also helps uh talking about uh the AI industry I think it is evolving and specifically I like how uh everyone is trying to use generative AI as part of their daily life uh I I can give an example of mine so dayto day I use a lot of generative a I I don't think so I can uh go back to the earlier World pre uh llm or generator AI for example uh we have uh uh generative AI licenses Enterprise licenses so I try to brainstorm or just try to uh make progress on my scripts or just try to see how can I make this uh process better by just having a chat with an llm also uh in my daily life I use uh uh personal messaging apps and they also include now nowadays with integration with llm so I it just a tap away that is has been helpful for example if I need to go to a trip I just can ask hey give me a packing list also I I drive a Tesla which has a FSD so I can never go back uh driving manually or just uh focusing I just tried to use FSD it's so good so I think generative AI uh or AI in general has become a good chunk of my life and I think I can't imagine my life without it right and everyone focus in AI about the future I really want to talk about the present can you share what are the challenges you see right now and what problems you would like to solve being an AI being an engineer yes I I think uh specifically from the adoption of AI standpoint I think people are still trying to use how or still trying to understand how they can incorporate AI or llms as part of their daily workflow uh so there there are ton of tools for developers like co-pilots uh for improving their uh coding workflow or coding uh pipeline but I think people are still experimenting uh nobody has come to a conclusion that hey this is the best llm this is the best product uh to accelerate your uh coding uh so I think the same with I feel like companies are still trying to understand uh how they can use generative AI as part of their product and also empowering their Workforce so I think it is still work in progress I think maybe next year we might have better understanding about how this goes uh so so just my understanding or just my intuition so it's a very open field there's like you know no set path created for one problem correct you have to figure out you have to yeah it's a cat and mouse game right as people say yeah yeah I think every product that you use today has some form of generative AI so do you want", "mimetype": "text/plain", "start_char_idx": 3955, "end_char_idx": 8794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3da9e42-42d1-4892-a064-20777ef24638": {"__data__": {"id_": "d3da9e42-42d1-4892-a064-20777ef24638", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05704e0b-aa2f-4a43-97fc-b4e42a5c4977", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "1627784215cffba8306944a56df584c0cea0a6bf8df1da2f06b6dd6186b57c8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "028495b3-8c67-4015-a34c-dc1351c4b57a", "node_type": "1", "metadata": {}, "hash": "2f7fe3c9c92472a3624ddc03d02be2d31f15b50f5cb66d69ba8f2de31e735811", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "can incorporate AI or llms as part of their daily workflow uh so there there are ton of tools for developers like co-pilots uh for improving their uh coding workflow or coding uh pipeline but I think people are still experimenting uh nobody has come to a conclusion that hey this is the best llm this is the best product uh to accelerate your uh coding uh so I think the same with I feel like companies are still trying to understand uh how they can use generative AI as part of their product and also empowering their Workforce so I think it is still work in progress I think maybe next year we might have better understanding about how this goes uh so so just my understanding or just my intuition so it's a very open field there's like you know no set path created for one problem correct you have to figure out you have to yeah it's a cat and mouse game right as people say yeah yeah I think every product that you use today has some form of generative AI so do you want to stick to just one uh AI app or just do you want to use generative AI within all the existing apps so but they don't share context with each other so that also might be a problem but I think yeah we are still figuring it out excellent so you have worked on the product side engineering side and you were in machine learning computer version from the beginning so can you share what is the difference between machine learning deep learning we used to do before the llm era versus now correct that's an interesting question I think uh uh the Deep learning era started with computer vision and that is where I think Nvidia GPU uh were explored or I think it was known that it works well with the AL Alex net uh introduction so I think before llm era there used to be lot more work around creating your application for example if you had to create a chart board there has there had to be much more work that uh needed to be done uh in order to fine tune uh the B model uh the B model from Google break it break it down yes yes so B so if you had to uh if you had to create a chart bot you had to uh first if if there's a response from a customer so you had to understand the text by breaking it down by removing all the stop words like the unnecessary Words which computer doesn't need to know like the uh in so you need to remove those then you need to understand the intent about what the intent about that phrase is so uh there was entity entity CL entity classification models that you were done and based on that you had you were coming up with a use case which was structured or which was just one of the five use cases in a customer uh Service chat bot so that is how the pipeline work but with the introduction of llm it just knows what so it understand the language so well that it can just directly answer like what is the use case that customer is looking for so you don't need to do all the pre-processing that was uh needed to be done in the pre llm or Transformers era and we have spreed up to a level that you know um those pre-training can now not just happen faster but in the class cloud with everything set up thanks to even Nims right Nvidia inference microservices that's the latest greatest that Nvidia released 2 years ago I watched the keynote big fan of Jensen hang how simplified simplified explanation he gave of nimes correct how would you explain names to the audience who are Engineers correct and who don't know anything about it yeah that's uh that's interesting and yeah I was part I went to the keynote uh saw it live it was amazing uh just that but uh so there's two ways so so there are two uh stages in any llm development pipeline one is in uh training and the second is inflence so training uh for training uh Nvidia has Nemo which we'll uh talk in depth later but focusing on the inference aspect inference is difficult or uh deployment is difficult and the reason being is so the all the models that you get uh for example uh model from llama 3.", "mimetype": "text/plain", "start_char_idx": 7820, "end_char_idx": 11779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "028495b3-8c67-4015-a34c-dc1351c4b57a": {"__data__": {"id_": "028495b3-8c67-4015-a34c-dc1351c4b57a", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3da9e42-42d1-4892-a064-20777ef24638", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "b56a66027d7dceb8597df623ff1d2237d039ab9a8be3ee4904f107cc987d83f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3e74fe2-a599-4d1c-93f1-ff92dbac9294", "node_type": "1", "metadata": {}, "hash": "2e7e563bf4eea65ce5861d589e3cd28f220d56996f26d551d25ba2efc5173cd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "up to a level that you know um those pre-training can now not just happen faster but in the class cloud with everything set up thanks to even Nims right Nvidia inference microservices that's the latest greatest that Nvidia released 2 years ago I watched the keynote big fan of Jensen hang how simplified simplified explanation he gave of nimes correct how would you explain names to the audience who are Engineers correct and who don't know anything about it yeah that's uh that's interesting and yeah I was part I went to the keynote uh saw it live it was amazing uh just that but uh so there's two ways so so there are two uh stages in any llm development pipeline one is in uh training and the second is inflence so training uh for training uh Nvidia has Nemo which we'll uh talk in depth later but focusing on the inference aspect inference is difficult or uh deployment is difficult and the reason being is so the all the models that you get uh for example uh model from llama 3.1 the latest model so they they have open weights available on hugging fees but you cannot just take those uh model weights and do start inferencing it is good for just a proof of concept or a demo but at the production scale it's not good because uh running that model would not get you the throw put that it requires for any production use case so all those models are in fp6 Precision so there are various Precision um uh uh available in deep learning so one of them is FP p16 which is floating 16 so which what does that mean is the number that are stored and computed are in floating 16 Precision so it is a 16 bit it would have like decimal level uh numbers which would be computed uh also during matrix multiplication and the problem with that is it is good for foundational model uh training or building but it is not good for inference because it requires much more compute uh at the inference side of things and to tackle that problem uh uh the usual method is to do quantization basically uh converting all the fp16 weights uh and activations to a smaller uh Precision in this case it could be integer 8 so it is integer only and 8 bit but you lose the decimal numbers uh in inate uh representation but there are various ways you can preserve uh those uh numbers or preserve the quality or the accuracy of the model by doing various kinds of techniques and uh but the upside is now you are not doing decimal computation so it is lot more easier and the numbers that are being uh computed or calculated in matrix multiplication it's just integer only so think of is like 7 8 9 so multiplying 7 into 8 is much easier than uh multiplying 7.12 3 into 8.6 3 4 so that's an example so uh that is one of the optimization that you can do to make the model run faster and then there's other optimization so you see there's a multiple step in order to like optimize a large language model to be production ready for inference for inference for inference right so uh Nim tries to uh make that process simpler by take making it easier or uh making it automatic for the developer so Nvidia uh the full form of name is NVIDIA inference microservices so you uh the idea is to uh you can basically get a microservice optimized container for any open source model that you can find on build. nvidia.com so there's almost I think there's two ways to access them so one is uh you can basically get uh or generate an API key like open AI and then basically use a open source model which is optimized and running on nvidia's cloud hosted infrastructure so uh for a for a free user I think you get th000 credits or th000 API calls for if you have an Enterprise account you uh or work email you get 500 uh 5,000 credits and but if you want to run that locally on your uh GPU so there's also a Docker container which you can uh basically follow the instruction on build. nvidia.com and just download it so once you download the docker container it starts executing it notices what is the uh Hardware that you are running so is it ampere based a100 is it Hopper based h100 and then it tries to download the optimized model for that specific hardware for your system and once it is downloaded the model is then uh hosted for inference and it gives gives you out a API endpoint which you can hit to uh get inference request or inference queries uh back so it is uh and it is easy to accommodate or just integrate that into your existing pipeline because it has a open AI API compatible server so if you are using open a API you can just try switching um uh with Nims and you should be good to go so let's say a company x correct they buys Nvidia Hardware h100 correct there is lot of training to use that Hardware which is $30,", "mimetype": "text/plain", "start_char_idx": 10795, "end_char_idx": 15490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3e74fe2-a599-4d1c-93f1-ff92dbac9294": {"__data__": {"id_": "d3e74fe2-a599-4d1c-93f1-ff92dbac9294", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "028495b3-8c67-4015-a34c-dc1351c4b57a", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "52807fbf29cc0525cc06812a4d17e161a42c9c3f35a52b7229e6d647457af143", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fd26ed5-2157-4be5-87a2-dafdb05b79a0", "node_type": "1", "metadata": {}, "hash": "0b2e30c0bd970a7a09254e413dd7c7e8a63eb8cddd783f166f68a399334bb5aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "nvidia.com and just download it so once you download the docker container it starts executing it notices what is the uh Hardware that you are running so is it ampere based a100 is it Hopper based h100 and then it tries to download the optimized model for that specific hardware for your system and once it is downloaded the model is then uh hosted for inference and it gives gives you out a API endpoint which you can hit to uh get inference request or inference queries uh back so it is uh and it is easy to accommodate or just integrate that into your existing pipeline because it has a open AI API compatible server so if you are using open a API you can just try switching um uh with Nims and you should be good to go so let's say a company x correct they buys Nvidia Hardware h100 correct there is lot of training to use that Hardware which is $30,000 lot of training needed to you know use that hardware for any model for inference correct and to do it locally on that Hardware correct because you want to keep the data private correct now that has become easier with using the Nim microservice correct and now with that architecture you can do it on nida's website on the cloud and have it ready API ready model ready and you can use it privately there is that right yeah and it is all optimized and developer doesn't have to do anything so it brings the deployment time from weeks to uh several minutes in some cases or just days yeah excellent and what future you think it has changed for companies in terms of profits and in terms of employees needed to run a company correct that's a good question so in that case I think it helps uh employees spend less time on optimizing and just more time in iterating model and just deploying it so uh the reason being uh if you are just using the traditional way which is which is good if you so if you are if you have a really huge scale and any uh any inference time saved or through latency time save brings you millions of dollars then it makes sense to spend more amount of time in optimizing your model for your own use case but if you are just using open source models uh then you can just use this uh brings the deployment time uh significantly and yeah I think brings I think makes them more productive and uh being more iteration heavy than than just uh spending so much time on a single model optimizing overall for developers it is saving a lot of time because let's say at the end of the day LM llms are just like math multiplication let's say linear algebra multiplication or you know matrix multiplication earlier if there are two matrices like 3x3 Matrix 3x3 Matrix two multiplied in two functions a * B 8 * 7 and 6 * 5 correct both happening in a different function now you can happen both can happen in the same function in a simplified way like forgetting about the floating point if was 7.5 * 6.5 we can simplify it to 8.7 all the simplified process can happen in nvidia's inference microservice correct so it saves a lot of time in compute for for an engineer correct you mentioned Nemo framework as well which is so big part of nvidia's Foundry as Jensen hang says that you know data centers are The Foundry they are the system you provide bread and butter for these llms to work and function that's basically The Foundry is right that's how Jensen H explains it and Nemo is very big part of it explain what Nemo is and how it impacts developers okay yeah so Nemo is nvidia's open source generative AI framework where you can build develop and train uh fine tune uh Speech AI uh language based and vision based uh models so it covers everything uh in the training aspect so uh and Nvidia gpus are a first class citizen or this library is built for NVIDIA gpus so all the things that you do as part of llm development is uh GPU accelerated from end to end uh for example uh the there are multiple components of Nemo framework so first is Nemo itself which helps with training but then there's also Nemo data curator so if you're trying to build your own custom large language model or build a foundational model or just trying to uh generate a data set for fine tuning you can use Nemo data curator which helps you generate uh uh data set but and process all the information from uh like which is internet scale kind of data and it is completely GPU accelerated and it is powered by uh Rapids CF so Rapids is GPU accelerated uh uh site of libraries which includes uh GP acceleration for pandas pyit lar and all those P py toch so p is covered by codn but rapid is specifically for data science and data is mainly data science like how can you do data processing on tabular kind of data so that was Nemo data curator uh Nemo also contains Nemo guardrails which is important nowadays so guardrails is an open source library library which helps you add guardrails to your uh llm application for example", "mimetype": "text/plain", "start_char_idx": 14637, "end_char_idx": 19507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fd26ed5-2157-4be5-87a2-dafdb05b79a0": {"__data__": {"id_": "9fd26ed5-2157-4be5-87a2-dafdb05b79a0", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3e74fe2-a599-4d1c-93f1-ff92dbac9294", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "e4d51fe683cf3d492886f00192d36ac873c68747981461dec3128d34e3939c1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b598c450-7339-42b4-ab24-283128ab6da3", "node_type": "1", "metadata": {}, "hash": "d7ac62498941505982ea05b25542615b8958c8de8ace3deb068238271060f8f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of Nemo framework so first is Nemo itself which helps with training but then there's also Nemo data curator so if you're trying to build your own custom large language model or build a foundational model or just trying to uh generate a data set for fine tuning you can use Nemo data curator which helps you generate uh uh data set but and process all the information from uh like which is internet scale kind of data and it is completely GPU accelerated and it is powered by uh Rapids CF so Rapids is GPU accelerated uh uh site of libraries which includes uh GP acceleration for pandas pyit lar and all those P py toch so p is covered by codn but rapid is specifically for data science and data is mainly data science like how can you do data processing on tabular kind of data so that was Nemo data curator uh Nemo also contains Nemo guardrails which is important nowadays so guardrails is an open source library library which helps you add guardrails to your uh llm application for example it will uh make sure that your llm application doesn't go out of uh the boundaries or set of boundaries that you have created for your application or it is making sure the response is toxic or uh it is it's not toxic and U there within the guidelines that you have provided or not it also makes sure that the answers generated are within uh the context uh that the uh question is provided so that was Nemo Gils then there's Nemo customizer as well which helps you do the fine-tuning aspect so there are tons of good uh components and that you can use to build and train large language model and of course for inference there's uh Nvidia inference microservices on NY lot to unpack there so let's say an engineer a developer wants to you know use an existing model how n helps in simple words correct correct that's a good step by step yeah that's a good question Nemo checkpoints or the Nemo the models that are trained with Nemo have a different uh uh different specification so I think the model trains with trained with pyo comes with PT kind of uh checkpoints whereas uh model trains with uh trained with Nemo comes with Nemo so it has different kind of uh training mechanism so it tries to make sure uh that you are using all the uh GPU resources that are available at your disposal uh including various kind of parallelism so if you have multiple gpus so how can you paralyze the training mechanism on multiple gpus so uh you don't have to work through whereas with other Frameworks uh GPU is supported but you need lot more expertise in order to paralyze that workload throughout the uh data center or the multiple gpus that you have and you provide this on Nims as well as so n is mainly for inference uh inference workload whereas Nemo is mainly focused for training and fine-tuning large language model okay so uh like starting from data processing to build uh for uh foundational large language models to fine tuning or customizing your model for your specific use case which could be Enterprise based or your customer related yeah excellent so Nemo accelerates the development of new models correct and Nims ACC Ates how you process data and inference quickly as quickly as possible correct yeah so think of training is um you can accelerate training with Nemo and inference with nyss excellent and I really want to ask you about fine tuning you mentioned that fine tuning is accelerated with Nemo right so basically what happens in fine-tuning is uh you provide a set of uh responses so question and answer you create that data set and try to uh create a layer on top of existing open source model and try to like uh influence the weight in such a weight and activation layer in such a way that it learns like what are the question and answers and what is the correlation between them so it tries to understand uh with the data set that you have provided uh with qu question and answers it learns that and it can can answer the question directly um whereas with I think the other option is using rag so but with rag they uh the llm has to like understand uh what the uh what the whole context or the whole document is about and then come up with answer I see what do you see in 20year Old Engineers who want to be part of your team in AI what skills you would like to see in them I think uh uh llms or generative a a has become so good nowadays that you can solve any problem uh by just uh chatting uh with that AI so uh which means that you lose the motivation to understand what is happening uh like how does the neural network work uh what are uh how does learning rate impact the model training process so things like that I feel like uh people or students should spend more time understanding what goes behind the scenes or what are the foundations of large language models and there are tons of courses out available out there from Nvidia from other uh uh other platforms like deep learning. so I uh I recommend taking those course in and really understanding like what's the math behind", "mimetype": "text/plain", "start_char_idx": 18516, "end_char_idx": 23514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b598c450-7339-42b4-ab24-283128ab6da3": {"__data__": {"id_": "b598c450-7339-42b4-ab24-283128ab6da3", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fd26ed5-2157-4be5-87a2-dafdb05b79a0", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "37b166d3ae1cb1a6f3966e5842a2f447894d8ac68379271308f7f36288ee418b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "616d4c7a-57ba-4022-b55b-21e019a44d50", "node_type": "1", "metadata": {}, "hash": "ca5d208e25729b73370a1e58dcd0315fb87bed141d3ca38eddd6b5b29bce6610", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "but with rag they uh the llm has to like understand uh what the uh what the whole context or the whole document is about and then come up with answer I see what do you see in 20year Old Engineers who want to be part of your team in AI what skills you would like to see in them I think uh uh llms or generative a a has become so good nowadays that you can solve any problem uh by just uh chatting uh with that AI so uh which means that you lose the motivation to understand what is happening uh like how does the neural network work uh what are uh how does learning rate impact the model training process so things like that I feel like uh people or students should spend more time understanding what goes behind the scenes or what are the foundations of large language models and there are tons of courses out available out there from Nvidia from other uh uh other platforms like deep learning. so I uh I recommend taking those course in and really understanding like what's the math behind it because I've seen some people don't like the math and just uh shut off uh learning AI or deep learning and then go to uh software engineering so uh but I think that is uh that is helpful to understand how uh models works that uh that is impacts your model usage or how you uh fine-tune the model or how you uh uh engineer The Prompt in such a way that you get your desired results if if you don't have that understanding then uh it makes uh makes it hard at the latest stage when you're doing the job and then you might need to like just go back and learn that again so I would focus like uh understanding the basic and or the foundation before just jumping on it what technical skills you would want in them in the Engineers okay okay so that's a good question I think awareness of all the tools uh open-source tools available out there like uh pie torch hugging face like you should know how to use them and uh if you want to train a model like what tool would you use uh if you want to fine tune a model what are the uh techniques to fine tune and what are the uh uh Frameworks or libraries are out there uh to use uh to do the fine-tuning process can you can you name some yeah so far fine tuning you can use uh hugging face uh it has a tool uh called Laura I think there's a function called Laura uh for uh Nvidia also has a Nemo customizer within Nemo faor uh so then that was that and then for training you can use py I I think you can use hugging face as well uh Nemo of course so I think there are tons of tool I think first having awareness and then maybe pick one uh out of of all the available tools and then trying it out on your own data set or just trying uh try building a demo project in order to like learn all this stuff what projects you would like to see in the resum\u00e9s that you will hire for AI teams okay okay that's an interesting question so I I don't like generic projects like chat llm chat Bots or rag I feel like uh those are easy to reproduce or recreate like anyone can do it after taking a course I feel like uh people or students should try to look a problem in their daily life uh that they have been facing and try to build a solution using llms and then maybe uh just come up with a project which can live on GitHub uh so what would you build if you were in their place 22 year old I feel I I would like to create a Chad bot like her which has uh her the movie Her the most popular yeah yeah yeah not the romance aspect but just uh managing the daily life like uh today's my appointment with my uh physician or an appointment with my AO uh optometrist so things like that uh uh uh the AI should be able to like remind me that hey I need to I have a flight tomorrow so please check in currently I think those tools are there but uh these are disintegrated having a one single tune or one llm which has the context of your calendar or maybe what are the upcoming appointment trips and things like that so that might be a fun uh project yeah that I would work on excellent explanation of what student should make I think her project is definitely doable in today's world with the resources now how you got good at llms and how you became llm expert at Nvidia correct so uh I already had a deep learning background through my uh undergrad in Pune yeah Pune yes so uh not specific Ally from the uh University but I was doing courses on the side and also coming to the states I did uh my masters uh in CS but it was focused on ML and I got to do bunch of internship throughout my time so it was all deep learning so I had my deep learning uh Basics covered uh but for llms I think it wasn't that hard going from uh deep learning to llm side of thing because uh I already had the understanding of", "mimetype": "text/plain", "start_char_idx": 22524, "end_char_idx": 27243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "616d4c7a-57ba-4022-b55b-21e019a44d50": {"__data__": {"id_": "616d4c7a-57ba-4022-b55b-21e019a44d50", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b598c450-7339-42b4-ab24-283128ab6da3", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "24c49c1d8b4c2bc362a5432c23f62497dfa55e9e7897559c2fc084070fc2f6b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a5d1190-de5c-46a1-8ddf-6b35d2120f66", "node_type": "1", "metadata": {}, "hash": "8bc7803a0775c1b529a4a72bece7c33f5d597bec3583bd86d6990bdc364e431e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "uh these are disintegrated having a one single tune or one llm which has the context of your calendar or maybe what are the upcoming appointment trips and things like that so that might be a fun uh project yeah that I would work on excellent explanation of what student should make I think her project is definitely doable in today's world with the resources now how you got good at llms and how you became llm expert at Nvidia correct so uh I already had a deep learning background through my uh undergrad in Pune yeah Pune yes so uh not specific Ally from the uh University but I was doing courses on the side and also coming to the states I did uh my masters uh in CS but it was focused on ML and I got to do bunch of internship throughout my time so it was all deep learning so I had my deep learning uh Basics covered uh but for llms I think it wasn't that hard going from uh deep learning to llm side of thing because uh I already had the understanding of Transformers U mainly coming from BT models but it was just a uh little different in the llm space so I took a bunch of courses available out there uh like uh student word from corsera uh and other uh learning platforms so that was my introduction and I as I was ramping up I I did some projects uh tried to understand how the all the tools work in the ecosystem and just tried uh running uh those on gpus so I did nothing basically took the examples that are that were given the courses or in the GitHub repositories from the tool makers or Library providers uh and just uh run them and try trying them on gpus and yeah what what's your most impactful work in llm uh that defines you as an expert okay okay that's an interesting question okay so uh I recently I worked on a multimodel AI rag uh project uh so uh the video is live on Nvidia developer YouTube channel so if someone wants to go in and see how it works so feel free to but yeah so it basically creates a uh multimodel AI rag pipeline or rag agent which can answer question uh based on your PDF PowerPoint presentation which can include charts plots uh images so so it extends uh rag with multimodal capabilities ingesting all uh various kinds of PDFs PP uh PPT popo presentation so excellent I think rag based pipelines had been very limited you know it was just Tech space and multimodality is excellent and what else if you'd like to name uh yeah so I am working on few more projects which I can't name but uh yeah stay tuned what what future you see with LMS and rag based systems right now so I feel like uh rag uh rag will evolve uh with of course the new direction is multimodal uh capability I think there's ton of research going on there uh the other thing with rag I feel like uh this model the smaller models will be much more useful so as the models are getting smaller it is easy and efficient to run rag pipelines so if model is good at reasoning I think that should be good enough to uh connect with the external data source or database and get the pipeline running than uh having a model which is Big which is which has the World Knowledge but uh in a Enterprise setting for this particular use case that World Knowledge isn't relevant right so just using a smaller model which has good uh reasoning capabilities connecting uh that to a large uh database or Source uh Source data uh of Enterprise I think that would be a good use case uh the other thing that I see is uh I think local co-pilots as uh as the model are getting smaller uh there's ton of optimization that uh can be done and if is being done which is making uh making them run on small devices like laptops and phones nowadays so I think that is a good Next Step that we are heading into so that we don't have to send our data to a server you can just run the models locally on uh a smartphone or laptop and get the results what are the challenges you see in your day-to-day life in llms and what misconceptions people have of you around you correct uh so uh talking about challenge is I feel like this uh the the field is moving so fast that it's sometimes hard to keep up what's happening and just uh trying to understand like I think there was a time uh in one or two weeks there was announcement of three or four open source models and then there was also uh Tech bunch of research or techniques that were was uh announced or released by few companies that how to make uh the context length bigger and better for model so I think in that case it's really hard to uh uh hard to understand like what thing should I focus on or also uh I'm also thinking about how can I educate about one technology to our developers right so picking one technology out of the spectrum uh becomes hard and also keeping up with it is also hard but I think I have my uh setup uh which has been helping me uh so I just try to follow a bunch of folks on Twitter and", "mimetype": "text/plain", "start_char_idx": 26282, "end_char_idx": 31132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a5d1190-de5c-46a1-8ddf-6b35d2120f66": {"__data__": {"id_": "0a5d1190-de5c-46a1-8ddf-6b35d2120f66", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "616d4c7a-57ba-4022-b55b-21e019a44d50", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "23dcddac811d380d0730c6a81d85152e35b267ddb25a2e14749a8de87ee472c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5671b6e9-3f01-4ee1-9674-dc4314cd7a29", "node_type": "1", "metadata": {}, "hash": "bbdd73904e22192f2989d30545de388ffb2b48409b7a717a28a41be7cace4eb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "life in llms and what misconceptions people have of you around you correct uh so uh talking about challenge is I feel like this uh the the field is moving so fast that it's sometimes hard to keep up what's happening and just uh trying to understand like I think there was a time uh in one or two weeks there was announcement of three or four open source models and then there was also uh Tech bunch of research or techniques that were was uh announced or released by few companies that how to make uh the context length bigger and better for model so I think in that case it's really hard to uh uh hard to understand like what thing should I focus on or also uh I'm also thinking about how can I educate about one technology to our developers right so picking one technology out of the spectrum uh becomes hard and also keeping up with it is also hard but I think I have my uh setup uh which has been helping me uh so I just try to follow a bunch of folks on Twitter and just try to see what they have posted or what uh what's new and what's better so that's uh one thing or one challenge I think uh also uh uh I I'm producing videos now so uh generating or producing videos uh is a new skill for me so that is new uh but your sessions I've have seen some of your sessions on YouTube yeah you you you teach pretty well about LMS and especially that rag project I think our our CEO says that uh you shouldn't be afraid afraid of AI I think you should embrace AI uh which means that you you should be able to or you should try to learn all these generative AI tools uh uh which improves the productivity of a developer or anyone using it so that they become super human and be Irreplaceable so that's a thing uh that uh I follow I think just Embrace this try to see how can I improve my workflow uh by using all these tools uh and get more productive rather than just avoiding it and being scared out of it the biggest misconception of any AI engineer is the fear they bring in the life of Engineers but but I think you know Jensen W says the in the best way that Engineers will no longer be Engineers they will become super humans or super Engineers right one thing I've seen in llms that is lacking is that let's say I want to customize for my own use case let's say I have an English paper I wrote you know for my students and 50 students submitted it correct every different model like of competitors like meta Google anthropic they all can give different points some give 9 out of 10 10 out of 10 11 out of 10 but if I want a system in AI I want to give let's say based on grammar I want to find fine tune it and put a criteria if grammar is excellent give 10 out of 10 and if let's say ke if if let's say the wording is not American English and British English deduct points I want to make my own metrics correct I cannot do that properly right now correct so what what you think the future is in this process driven development is that in the talks right now so I think there's three ways to approach this problem so the first or uh the easiest one is uh using prompt engineering so uh what does this mean is you uh create a prompt in such a way so descriptive that the model knows how to grade uh these uh uh questions or Answers by giving them uh by giving it a few examples about how would you do it so uh if this is the answer I would rate this as six is this is the answer so you create a basically a prompt before uh asking llm to judge or uh query or just uh submit a uh assessment for a question so that's the easiest way but the problem with this is it is not uh consistent yes that's the problem yeah and the second one is you are increasing the context length of the model so if you're running this at scale this might become a problem so the second one is you fine-tune the model uh using one of uh the techniques which is uh for example Laura so which is very efficient in terms of fine tuning so you basically create a data set about uh responses and grade so you create that data set and try to re uh try to train a model with those data set and get uh it will give you a fine tune model which you can use uh to assess and the third or the most uh uh I think expensive ways to create a foundational model from scratch just for this purpose so I think based on this I think the finetuning aspect uh just creating a data set about how would you rate uh answers I think that would be a better option and the tools are available out there like hugging face and it is also GPU accelerated so you can use openly or freely available gpus like T4 uh with I think that would support only a certain size of model uh but I think that will still will get you started right and a lot of Engineers watching me", "mimetype": "text/plain", "start_char_idx": 30162, "end_char_idx": 34877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5671b6e9-3f01-4ee1-9674-dc4314cd7a29": {"__data__": {"id_": "5671b6e9-3f01-4ee1-9674-dc4314cd7a29", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a5d1190-de5c-46a1-8ddf-6b35d2120f66", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "37ccf80d6f1a80d5b11845d8d652c5f15981611b0f59da8475ea82a7b776e23f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60f009dd-392d-4eb8-a74c-ea200c9ec05d", "node_type": "1", "metadata": {}, "hash": "7d05dcaf880e7cdc7847cf457ae72ab16a7702950e3e3ba28510a9c35a822cfc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "one is you fine-tune the model uh using one of uh the techniques which is uh for example Laura so which is very efficient in terms of fine tuning so you basically create a data set about uh responses and grade so you create that data set and try to re uh try to train a model with those data set and get uh it will give you a fine tune model which you can use uh to assess and the third or the most uh uh I think expensive ways to create a foundational model from scratch just for this purpose so I think based on this I think the finetuning aspect uh just creating a data set about how would you rate uh answers I think that would be a better option and the tools are available out there like hugging face and it is also GPU accelerated so you can use openly or freely available gpus like T4 uh with I think that would support only a certain size of model uh but I think that will still will get you started right and a lot of Engineers watching me you know they think that llms are expensive NLP ways you know before llms are still cheaper than using an using like you know apis of these large langu models because for large scales they become still very expensive mhm so do you know some hacks or ways to reduce the cost per token or like to use a prompt that uses less tokens mhm mhm in LMS okay okay okay I think I feel like uh this is a prompt engineering question and it is more art uh than science but what I've seen is uh being descriptive helps and there are tons of courses out out there available on how you can uh create a a specific prompt that will get you I don't have any like specific answers but I think just try to take one of those courses uh but what I've seen is being descriptive really helps yeah what security risk and threats you see while building an llm using an llm inferencing an llm in all three stages I think uh mainly the security risk is at the deployment side uh I think one of the examples is prompt injection where you just basically ask a a customer chart board to do a non-customer service or use case and that uh in some cases it answers that question so I think it is much more uh recommended to have guard rails around your uh use case or uh deployment uh for which Nvidia has Nemo guard rails which helps you do that but there's are other open source tools available as well how does it work how do guard work so you basically uh try to see and judge all the queries that or the transaction happen between user and the llm and uh the Nemo guardrail tries to check if the response generated is within within the boundaries like it is not toxic and it or it is not going uh outside of the use case so there's multip like there's ton of documentation but it just makes sure that uh the llm response are generated without within a guard rail or within a boundary that the application is supposed to do and not go outside of Hardware G right no so these are not Hardware these are software uh so you basically H write a a specification about uh what what your needs are so there are various parameters uh so there's documentation available so security threats are more on the Infant side M so that we need to we need some guard rails so that we can protect the answer of that prompt comes related to what you asked and software level guard rails are Nemo is part of Nemo guard rails Nemo guard rails yeah now any other threats you see that we need to be cautious about for all the startup Founders watching this or AI Engineers watching this yeah I think just I think the Gil would be the most important uh thing for now all so uh I think there's there are various tools out there in terms of generative AI so I think we should be uh we should make sure that what kind of data is being sent uh to the clouds while processing if they are uh keeping their if they are keeping our data onto their servers or not so things like that should be uh more uh we should be more careful of using those tools awesome and I heard that there is NVIDIA conference coming up in India can you please share details on that yes yes yes yeah so uh there's a uh conference called Nvidia AI Summit uh that is upcoming in India uh it would be in Mumbai at the Geo uh World convention center and Jensen will be there uh having a fireside chat so and there will be ton of good sessions as well uh uh ranging from data processing uh to llms and everything in between so uh I would recommend your audience to join that and uh uh your audience will also get a 20% discount on the conference price uh so that I'll share it and you can put that into the YouTube description awesome thank you so much Jay it was wonderful talking to you learned a lot as always and I hope this video inspires more Engineers AI enthusiasts to devel to do development the right way thank you so much again using Nemo and Nims thank you again thank", "mimetype": "text/plain", "start_char_idx": 33928, "end_char_idx": 38765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60f009dd-392d-4eb8-a74c-ea200c9ec05d": {"__data__": {"id_": "60f009dd-392d-4eb8-a74c-ea200c9ec05d", "embedding": null, "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc", "node_type": "4", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "943be39e85e6c7b58bd6541c0a7d77455c2ffa206c4f55914ecbafaac5712f54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5671b6e9-3f01-4ee1-9674-dc4314cd7a29", "node_type": "1", "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}, "hash": "d788bca8e2cad684110615c6b530869c29e032f9e8849da0caad831a7931f517", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "things like that should be uh more uh we should be more careful of using those tools awesome and I heard that there is NVIDIA conference coming up in India can you please share details on that yes yes yes yeah so uh there's a uh conference called Nvidia AI Summit uh that is upcoming in India uh it would be in Mumbai at the Geo uh World convention center and Jensen will be there uh having a fireside chat so and there will be ton of good sessions as well uh uh ranging from data processing uh to llms and everything in between so uh I would recommend your audience to join that and uh uh your audience will also get a 20% discount on the conference price uh so that I'll share it and you can put that into the YouTube description awesome thank you so much Jay it was wonderful talking to you learned a lot as always and I hope this video inspires more Engineers AI enthusiasts to devel to do development the right way thank you so much again using Nemo and Nims thank you again thank you for having me", "mimetype": "text/plain", "start_char_idx": 37780, "end_char_idx": 38783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"8be2311a-a564-44b3-a7cd-298ba039a20f": {"doc_hash": "8db88086e4347b83baf80d8323cd40942c70dc66345b62d0503e7d9fac98f18e", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "05704e0b-aa2f-4a43-97fc-b4e42a5c4977": {"doc_hash": "1627784215cffba8306944a56df584c0cea0a6bf8df1da2f06b6dd6186b57c8d", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "d3da9e42-42d1-4892-a064-20777ef24638": {"doc_hash": "b56a66027d7dceb8597df623ff1d2237d039ab9a8be3ee4904f107cc987d83f3", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "028495b3-8c67-4015-a34c-dc1351c4b57a": {"doc_hash": "52807fbf29cc0525cc06812a4d17e161a42c9c3f35a52b7229e6d647457af143", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "d3e74fe2-a599-4d1c-93f1-ff92dbac9294": {"doc_hash": "e4d51fe683cf3d492886f00192d36ac873c68747981461dec3128d34e3939c1f", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "9fd26ed5-2157-4be5-87a2-dafdb05b79a0": {"doc_hash": "37b166d3ae1cb1a6f3966e5842a2f447894d8ac68379271308f7f36288ee418b", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "b598c450-7339-42b4-ab24-283128ab6da3": {"doc_hash": "24c49c1d8b4c2bc362a5432c23f62497dfa55e9e7897559c2fc084070fc2f6b4", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "616d4c7a-57ba-4022-b55b-21e019a44d50": {"doc_hash": "23dcddac811d380d0730c6a81d85152e35b267ddb25a2e14749a8de87ee472c9", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "0a5d1190-de5c-46a1-8ddf-6b35d2120f66": {"doc_hash": "37ccf80d6f1a80d5b11845d8d652c5f15981611b0f59da8475ea82a7b776e23f", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "5671b6e9-3f01-4ee1-9674-dc4314cd7a29": {"doc_hash": "d788bca8e2cad684110615c6b530869c29e032f9e8849da0caad831a7931f517", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}, "60f009dd-392d-4eb8-a74c-ea200c9ec05d": {"doc_hash": "7630f4efbac6a8f560eef140095719ace0d4e823d42e98cc0180df11f3695141", "ref_doc_id": "f0fa43f5-0b75-443a-83c3-6d4c3c469bcc"}}, "docstore/ref_doc_info": {"f0fa43f5-0b75-443a-83c3-6d4c3c469bcc": {"node_ids": ["8be2311a-a564-44b3-a7cd-298ba039a20f", "05704e0b-aa2f-4a43-97fc-b4e42a5c4977", "d3da9e42-42d1-4892-a064-20777ef24638", "028495b3-8c67-4015-a34c-dc1351c4b57a", "d3e74fe2-a599-4d1c-93f1-ff92dbac9294", "9fd26ed5-2157-4be5-87a2-dafdb05b79a0", "b598c450-7339-42b4-ab24-283128ab6da3", "616d4c7a-57ba-4022-b55b-21e019a44d50", "0a5d1190-de5c-46a1-8ddf-6b35d2120f66", "5671b6e9-3f01-4ee1-9674-dc4314cd7a29", "60f009dd-392d-4eb8-a74c-ea200c9ec05d"], "metadata": {"file_path": "data/youtube/Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_name": "Meet_LLMs_Expert_at_NVIDIA!_Ft._Jay_Rodge!.txt", "file_type": "text/plain", "file_size": 38784, "creation_date": "2024-12-03", "last_modified_date": "2024-11-11"}}}}